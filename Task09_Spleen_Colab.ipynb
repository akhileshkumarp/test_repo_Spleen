{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981e680e",
   "metadata": {},
   "source": [
    "# Task09 Spleen - Google Colab\n",
    "This notebook runs training and testing for the Task09_Spleen project.\n",
    "\n",
    "**Free tier tips**\n",
    "- Prefer GPU runtime if available.\n",
    "- Start with patch size 48 and batch size 1; increase if memory allows.\n",
    "- Validation can be slow on CPU.\n",
    "\n",
    "**Steps**\n",
    "1. Upload or sync the dataset to Google Drive.\n",
    "2. Set the project directory.\n",
    "3. Run training and optional testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and install dependencies\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print('Python', sys.version)\n",
    "try:\n",
    "    import torch\n",
    "    print('Torch', torch.__version__)\n",
    "    print('CUDA available:', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print('GPU:', torch.cuda.get_device_name(0))\n",
    "except Exception as exc:\n",
    "    print('Torch import failed:', exc)\n",
    "\n",
    "# Colab usually includes torch. Install other deps.\n",
    "!pip -q install nibabel tqdm numpy\n",
    "\n",
    "# Optional: show GPU status\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (recommended)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set your project directory on Drive\n",
    "# Example: /content/drive/MyDrive/Task09_Spleen\n",
    "PROJECT_DIR = '/content/drive/MyDrive/Task09_Spleen'\n",
    "os.chdir(PROJECT_DIR)\n",
    "print('CWD:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8149c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "import json\n",
    "\n",
    "for name in ['dataset.json', 'imagesTr', 'labelsTr']:\n",
    "    exists = os.path.exists(name)\n",
    "    print(f'{name}:', 'OK' if exists else 'MISSING')\n",
    "\n",
    "if os.path.exists('dataset.json'):\n",
    "    with open('dataset.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print('Training cases:', len(data.get('training', [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (free tier defaults)\n",
    "# If you hit OOM, reduce patch_size to 32 or 48.\n",
    "!python train.py --data_dir . --epochs 5 --batch_size 1 --patch_size 48 48 48 --num_workers 2 --threads 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: run test/inference if available\n",
    "# !python test.py --data_dir . --checkpoint outputs/checkpoint_best.pt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
